<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sthossain.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://sthossain.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-12T14:07:07+00:00</updated><id>https://sthossain.github.io//feed.xml</id><title type="html">blank</title><subtitle>Website of Sk. Tanvir Hossain. </subtitle><entry><title type="html">Subgaussian Random Variables</title><link href="https://sthossain.github.io//blog/2024/distill/" rel="alternate" type="text/html" title="Subgaussian Random Variables"/><published>2024-09-08T00:00:00+00:00</published><updated>2024-09-08T00:00:00+00:00</updated><id>https://sthossain.github.io//blog/2024/distill</id><content type="html" xml:base="https://sthossain.github.io//blog/2024/distill/"><![CDATA[<p>When I started to read multi armed bandit (a.k.a. sequential treatment allocation) literature, had to deal with concentration bounds (somehow I love and hate this topic). The key idea is essentially bounding the tail probabilities of the random variables. Since I myself sometimes forget many details, I decided to write <del>everyday</del>, time to time (even if it’s very little). Hopefully by this I will learn many things and this helps people who has struggled like me.</p> <p>This is the first post of the series of posts on concentration bounds that I will try to write. There are different but equivalent ways to define a subgaussain random variable, I will follow following definitions,</p> <h2 id="basic-definitions-and-properties">Basic Definitions and Properties</h2> <blockquote> <p><strong>Definition 1 [Subgaussain random variable]:</strong> We say a random variable is a subgaussain random variable with subgausian parameter $\sigma$, we write $X \sim s\mathcal{G}(\sigma)$, if following holds \(\mathbb{E}(e^{\lambda X}) \leq e^{\frac{1}{2} \lambda^2 \sigma^2}\)</p> </blockquote> <p>Now I will quickly state some important properties of subgaussian random variables, that is possible to derive using this definition,</p> <p><strong>Proprties of Subgaussian Random Variables</strong></p> <ol> <li> Using Markov's inequality, we can show that for any $\epsilon &gt; 0$, $$ \mathbb{P}(X \geq \epsilon) \leq e^{-\frac{\epsilon^2}{2 \sigma^2}} \quad \text{ and } \quad \mathbb{P}(|X| \geq \epsilon) \leq 2 e^{-\frac{\epsilon^2}{2 \sigma^2}} $$ this is often called **Chernoff bound**, and is very useful in many places. The important thing to note where subgaussian paramter $\sigma^2$ is in the bound. </li> <li> If $X_i \sim s\mathcal{G}(\sigma_i)$ and all of the random variables are independent, then $$ \sum_{i = 1}^{n} X_i \sim s\mathcal{G}\left(\sqrt{\sum_{i = 1}^{n}\sigma_i^2}\right) $$ A special case of this is when all $\sigma_i$ are same, i.e., $\sigma_i = \sigma$ then we have a iid setup, or we have $X_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma)$, and we have $\sum_{i = 1}^{n} X_i \sim s\mathcal{G}(\sqrt{n} \sigma)$ </li> <li> If $X \sim s \mathcal{G}(\sigma)$, then $$ a X \sim s \mathcal{G}(|a|\sigma) $$ </li> <li> From the last two results when $X_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma)$, we have $\bar{X}_n \sim s\mathcal{G}(\frac{\sigma}{\sqrt{n}})$, where ${\bar{X}_n := \frac{1}{n}\sum_{i = 1}^{n} X_i}$. In this case, applying the result from property 1, or Chernoff bound, we get for any $\epsilon &gt; 0$, $$ \mathbb{P}(\bar{X}_n \geq \epsilon) \leq e^{-\frac{n \epsilon^2}{2 \sigma^2}} $$ this is a very important result, which we will use in many places. </li> <li> Many random variables are subgaussian, for example, <ul> <li> If $X \sim \mathcal{N}(\mu, \sigma^2)$, then $$ X - \mu \sim s\mathcal{G}(\sigma) $$ so in this case the $\sigma$ is the standard deviation of the normal random variable. </li> <li> Bounded Random Variable $X$, for example $X \in [a, b]$ then $$ X - \mu \sim s\mathcal{G}\left(\cfrac{b-a}{2}\right) $$ </li> </ul> </li> </ol> <h2 id="hoeffdings-inequality">Hoeffding’s Inequality</h2> ]]></content><author><name></name></author><category term="concentration_bounds"/><category term="subgaussain_random_variables"/><summary type="html"><![CDATA[Some details on Subgaussian]]></summary></entry></feed>