<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sthossain.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://sthossain.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-08T18:14:08+00:00</updated><id>https://sthossain.github.io//feed.xml</id><title type="html">blank</title><subtitle>Website of Sk. Tanvir Hossain. </subtitle><entry><title type="html">Subgaussian Random Variables</title><link href="https://sthossain.github.io//blog/2024/distill/" rel="alternate" type="text/html" title="Subgaussian Random Variables"/><published>2024-09-08T00:00:00+00:00</published><updated>2024-09-08T00:00:00+00:00</updated><id>https://sthossain.github.io//blog/2024/distill</id><content type="html" xml:base="https://sthossain.github.io//blog/2024/distill/"><![CDATA[<p>When I started to read multi armed bandit (a.k.a sequential treatment allocation) literature, again I had to deal with concentration bounds (I have a love and hate relationship with this). The key idea is essentially bounding the tail probabilities of the random variables.</p> <p>Since I myself sometimes forget many details, I decided to write everyday (even if this is very short) about the concentration bounds. Hopefully by this I will also learn many things in this area. This is the first post of the series of posts on concentration bounds that I will try to write.</p> <p><strong>References:</strong> There are many nice books, I will share some details. For example you can see a short and quick overview of the basic techniques in Lattimore, T., &amp; Szepesvári, C. (2020) <d-cite key="lattimore2020bandit"></d-cite></p> <h2 id="basic-definition-and-properties">Basic Definition and Properties</h2> <p>There are different but equivalent ways to define a subgaussain random variable, I will follow following definitions,</p> <p><strong>Definition 1 [Subgaussain random variable]:</strong> We say a random variable is a subgaussain random variable with subgausian parameter $\sigma^2$ if following holds</p> \[\mathbb{E}(e^{\lambda X}) \leq e^{\frac{1}{2} \lambda^2 \sigma^2}\] <p>We write this with $X \sim s \mathcal{G}(\sigma) $</p> <p>Now I will quickly state some important things that is possible to derive using this definition,</p> <ul> <li>Using Markov’s inequality, we can show that for any $\epsilon &gt; 0$,</li> </ul> \[\mathbb{P}(|X| \geq \epsilon) \leq 2 e^{-\frac{\epsilon^2}{2 \sigma^2}}\] <ul> <li>If $X_1 \sim s\mathcal{G}(\sigma_1)$ and $X_2 \sim s\mathcal{G}(\sigma_2)$ then $X_1 + X_2 \sim s\mathcal{G}(\sqrt{\sigma_1^2 + \sigma_2^2})$</li> </ul> ]]></content><author><name></name></author><category term="concentration_bounds"/><category term="subgaussain_random_variables"/><summary type="html"><![CDATA[Some details on Subgaussian]]></summary></entry></feed>