<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sthossain.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://sthossain.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-14T05:38:04+00:00</updated><id>https://sthossain.github.io//feed.xml</id><title type="html">blank</title><subtitle>Website of Sk. Tanvir Hossain. </subtitle><entry><title type="html">Subgaussian Random Variables</title><link href="https://sthossain.github.io//blog/2024/distill/" rel="alternate" type="text/html" title="Subgaussian Random Variables"/><published>2024-09-08T00:00:00+00:00</published><updated>2024-09-08T00:00:00+00:00</updated><id>https://sthossain.github.io//blog/2024/distill</id><content type="html" xml:base="https://sthossain.github.io//blog/2024/distill/"><![CDATA[<p>When I started to read multi armed bandit (a.k.a. sequential treatment allocation) literature, had to deal with concentration bounds (somehow I love and hate this topic). The key idea is essentially bounding the tail probabilities of the random variables. Since I myself sometimes forget many details, I decided to write <del>everyday</del>, time to time (even if it’s very little). Hopefully by this I will learn many things and this helps people who has struggled like me.</p> <p>This is the first post of the series of posts on concentration bounds that I will try to write. There are different but equivalent ways to define a subgaussain random variable, I will follow following definitions,</p> <h2 id="basic-definitions-and-properties">Basic Definitions and Properties</h2> <p>========================================================================</p> <p><strong>Definition 1 [Subgaussain random variable]:</strong></p> <p>We say a random variable is a subgaussain random variable with subgausian parameter $\sigma$, we write $X \sim s\mathcal{G}(\sigma)$, if following holds</p> \[\mathbb{E}(e^{\lambda X}) \leq e^{\frac{1}{2} \lambda^2 \sigma^2}\] <p>========================================================================</p> <p>Now I will quickly state some important properties of subgaussian random variables, that is possible to derive using this definition,</p> <p><strong>Proprties of Subgaussian Random Variables</strong></p> <ol> <li> Using Markov's inequality, we can show that for any $\epsilon &gt; 0$, $$ \mathbb{P}(X \geq \epsilon) \leq e^{-\frac{\epsilon^2}{2 \sigma^2}} \quad \text{ and } \quad \mathbb{P}(|X| \geq \epsilon) \leq 2 e^{-\frac{\epsilon^2}{2 \sigma^2}} $$ this is often called **Chernoff bound**, and is very useful in many places. The important thing to note where subgaussian paramter $\sigma^2$ is in the bound. </li> <li> If $X_i \sim s\mathcal{G}(\sigma_i)$ and all of the random variables are independent, then $$ \sum_{i = 1}^{n} X_i \sim s\mathcal{G}\left(\sqrt{\sum_{i = 1}^{n}\sigma_i^2}\right) $$ A special case of this is when all $\sigma_i$ are same, i.e., $\sigma_i = \sigma$ then we have a iid setup, or we have $X_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma)$, and we have $\sum_{i = 1}^{n} X_i \sim s\mathcal{G}(\sqrt{n} \sigma)$ </li> <li> If $X \sim s \mathcal{G}(\sigma)$, then $$ a X \sim s \mathcal{G}(|a|\sigma) $$ </li> <li> Many random variables are subgaussian, for example, <ul> <li> If $X \sim \mathcal{N}(\mu, \sigma^2)$, then $$ X - \mu \sim s\mathcal{G}(\sigma) $$ so in this case the $\sigma$ is the standard deviation of the normal random variable. </li> <li> Bounded Random Variable $X$, for example $X \in [a, b]$ then $$ X - \mu \sim s\mathcal{G}\left(\cfrac{b-a}{2}\right) $$ </li> </ul> </li> </ol> <h2 id="hoeffdings-bound">Hoeffding’s Bound</h2> <p>The idea of the Hoeffding’s Bound is to get a tail bound for the average / sum of the subgaussian random variables, i.e., we are looking for</p> \[\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \text{?}\] <p>or more simply,</p> \[\mathbb{P}\left[ \overline{X}_{n} - \mu \geq \epsilon \right] \leq \text{?}\] <p>In this case the key step is to first derive the subgaussian parameter of $\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right)$ or $\overline{X}_n - \mu$. This might vary depending on the setup. But if we know this, for example, if we know</p> \[\overline{X}_n - \mu \sim s\mathcal{G}\left(\tilde{\sigma}_{S}\right)\] <p>then applying Chernoff bound, we get</p> \[\mathbb{P}\left[\overline{X}_n - \mu \geq \epsilon \right] \leq \exp{\left(-\frac{1}{2} \cdot \frac{\epsilon^2}{ \tilde{\sigma}_S^2}\right)}\] <p>We will see some specific examples first and then outline the porposition from the book by Wainwright <d-cite key="wainwright2019high"></d-cite>.</p> <hr/> <p><strong>[Example 1]</strong></p> <p>If we have $X_i - \mu_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma_i)$, then we have</p> \[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \sim s\mathcal{G}\left(\frac{1}{n}\sqrt{\sum_{i = 1}^{n}\sigma_i^2}\right)\] <p>and applying Chernoff bound, we get for all $\epsilon \geq 0$,</p> \[\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \exp{\left(- \frac{1}{2}\cdot \frac{n^2 \epsilon^2}{\sum_{i=1}^n \sigma_i^2}\right)}\] <hr/> <p><strong>[Example 2]</strong></p> <p>if we have $X_i - \mu \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma)$, then we have</p> \[\overline{X}_n - \mu \sim s\mathcal{G}\left(\frac{\sigma}{\sqrt{n}}\right)\] <p>And applying Chernoff bound, we get for all $\epsilon \geq 0$, \(\mathbb{P}(\overline{X}_n - \mu \geq \epsilon) \leq \exp{\left(-\frac{1}{2}\frac{n \epsilon^2}{\sigma^2}\right)}\)</p> <hr/> <p><strong>[Example 3]</strong></p> <p>if all $X_i$ are bounded random variables where $X_i \in [a_i, b_i]$ , then we have</p> \[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \sim s\mathcal{G}\left(\frac{1}{n}\sqrt{ \sum_{i =1}^{n} \frac{(b_i - a_i)^2}{4} }\right)\] <p>And applying Chernoff bound, we get for all $\epsilon \geq 0$,</p> <p>\(\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \exp{\left(- \frac{1}{2}\cdot \frac{n^2 \epsilon^2}{\sum_{i=1}^n \frac{(b_i - a_i)^2}{4}}\right)} = \exp{\left(- \frac{2 n^2 \epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)}\) —</p> <p><strong>[Example 4]</strong></p> <p>Again another special case when $X_i \in [a, b]$, then we have</p> \[\overline{X}_n - \mu \sim s\mathcal{G}\left(\frac{1}{\sqrt{n}}\frac{(b-a)}{2}\right)\] <p>And applying Chernoff bound, we get for all $\epsilon \geq 0$,</p> <p>$$ \mathbb{P}(\overline{X}_n - \mu \geq \epsilon) \leq \exp{\left( -\frac{2 n \epsilon^2}{ (b-a)^2 } \right)}</p> <h2>$$</h2> <p>These are all examples of Hoeffding’s bound. Following is the general form of the Hoeffding’s bound, proposition is taken from the book of Wainwright <d-cite key="wainwright2019high"></d-cite></p> <blockquote> <p><strong>Proposition [Hoeffding bound</strong>]: Suppose we have a sequence $X_i - \mu_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma_i)$, then for all $\epsilon \geq 0$, we have</p> \[\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \exp{\left(- \frac{1}{2}\cdot \frac{n^2 \epsilon^2}{\sum_{i=1}^n \sigma_i^2}\right)}\] </blockquote> <p>Finally if $X_i \in [0, 1]$, for all $i$ and they are independent, we have a very simple bound,</p> \[\mathbb{P}(\overline{X}_n - \mu \geq \epsilon) \leq \exp{\left( - 2 n \epsilon^2 \right)}\]]]></content><author><name></name></author><category term="concentration_bounds"/><category term="subgaussain_random_variables"/><summary type="html"><![CDATA[Some details on Subgaussian]]></summary></entry></feed>