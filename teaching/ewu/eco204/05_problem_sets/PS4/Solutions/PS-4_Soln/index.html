<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body><p># $\S.$ Problem 1 Soln ## Solution a. Here $\hat{\beta}_1 = 10$, means holding all other variables constant, if $x_1$ increases by 1 unit, $Y$ is predicted to increase by 10 units. Similarly, $\hat{\beta}_2 = 8$ means holding all other variables constant, if $x_2$ increases by 1 unit, $Y$ is predicted to increase by $8$ units. Finally, $\hat{\beta}_3 = 9$ means holding all other variables constant, if $x_3$ increases by 1 unit, $Y$ is predicted to increase by 9 units. ## Solution b. ```{r} # define the function y_hat &lt;- function(x1, x2, x3){ output &lt;- 25 + 10*x1 + 8*x2 + 9*x3 return(output) } # test the function y_hat(2, 1, 5) ``` ## Solution c. ```{r} # predict Y y_hat(15, 10, 5) ``` We can also give the names of the arguments, the benefit is that we don't have to remember the order of the arguments. ```{r} # predict Y y_hat(x2 = 10, x1 = 15, x3 = 5) ``` # $\S.$ Problem 3 Soln Let's calculate SSE first ```{r} # SSR SST &lt;- 6724.125 SSR &lt;- 6216.375 SSE &lt;- SST - SSR SSE ``` We can calculate MSR with the following formula $$ \text{MSR} = \frac{\text{SSR}}{p} $$ ```{r} # MSR p &lt;- 2 # already this is defined before MSR &lt;- SSR / p MSR ``` Now we can also calculate MSE with the following formula $$ \text{MSE} = \frac{\text{SSE}}{n-p-1} $$ ```{r} n &lt;- 10 p &lt;- 2 MSE &lt;- SSE / (n - p - 1) MSE ``` Finally we can calculate $F$ with the following formula $$ F = \frac{\text{MSR}}{\text{MSE}} $$ ```{r} Fcalc &lt;- MSR / MSE Fcalc ``` Now to do the F-test we need to find the critical value from the $F$ distribution with $p = 2$ and $n-p-1 = 7$ degrees of freedom. We can use the `qf` function to find the critical value for the F-test. Here $n = 10$. ```{r} # find the critical value of F alpha &lt;- 0.05 n &lt;- 10 p &lt;- 2 # F test is always one tail test, this is F(1-alpha) Fcrit &lt;- qf(p = 1 - alpha, df1 = p, df2 = n - p - 1) Fcalc &gt; Fcrit ``` Since $F_{calc} &gt; F_{crit}$, we reject the null hypothesis. Recall the Null hypothesis here is $$ H_0: \beta_1 = \beta_2 = 0 $$ So this means at least one of the coefficients is non-zero. Now similarly we can do the t-test for $\beta_1$ and $\beta_2$. The t-test for $\beta_1$ is ```{r} # t-test for beta_1 tcalc_beta1 &lt;- (0.5906 - 0) / 0.0813 # it's a two tail test, so let's calculate the p value directly pvalue_beta1 &lt;- 2 * (1 - pt( abs(tcalc_beta1) , df = n - p - 1) ) pvalue_beta1 pvalue_beta1 &lt; alpha ``` So we reject the Null hypothesis that $\beta_1 = 0$. Similarly, we can do the t-test for $\beta_2$. ```{r} # t-test for beta_2 tcalc_beta2 &lt;- (0.4980 - 0) / 0.0567 pvalue_beta2 &lt;- 2 * (1 - pt( abs(tcalc_beta2) , df = n - p - 1) ) pvalue_beta2 pvalue_beta2 &lt; alpha ``` So we reject the Null hypothesis that $\beta_2 = 0$. # $\S.$ Problem 6 Soln ## Solution a. In answer a. we need to load the data, fit the simple linear regression model with `tv`, and then write the estimated regression equation. ```{r} # load the library library(readxl) # directly load the data Showtime &lt;- read_excel("/home/tanvir/Documents/ownCloud/Git_Repos/EWU_repos/3_Fall_2023/eco_204/ewu-eco204.github.io/pdf_files/ps/04_mlr/Solutions/Showtime.xlsx") ``` We can view the data ```{r} Showtime ``` It's also good to see some summary stats ```{r} summary(Showtime) ``` Looks okm Now let's regress `revenue` on `tv` ```{r, message = FALSE, warning = FALSE} # fit the model model_slr &lt;- lm(revenue ~ tv, data = Showtime) # view the results with stargazer package library(stargazer) stargazer(model_slr, type = "text") ``` So the estimated regression equation is $$ \widehat{revenue} = 88.63 + 1.604 \text{tv} $$ ## Solution b. We need to develop an estimated equation using both tv and newspaper advertising. So let's fit MLR (notice the dot after the tilde sign, this means all the other variables in the data set except the dependent variable revenue will be used in the model) ```{r} # fit the model model_mlr &lt;- lm(revenue ~ ., data = Showtime) # view the results with stargazer package library(stargazer) stargazer(model_mlr, type = "text") ``` So the estimated regression equation is $$ \widehat{revenue} = 82.010 + 1.952 \text{tv} + 1.243 \text{newspaper} + 0.311 \text{magazines} + 0.537 \text{leaflets} $$ ## Solution c. The two estimated coefficients of tv are different. I am not writing the interpretation but you should be able to do this (see the slides) ## Solution d. For multiple linear regression getting anova table in R requires a bit more work. If we run simple `anova(model_mlr)` function, this will not work (you can try!). We need to put two regression model in `anova()` ```{r} # fit null model model_null &lt;- lm(revenue ~ 1, data = Showtime) # we already have the complete model # anova function will compare between two models anova(model_null, model_mlr) ``` Careful here RSS (Residual sum of squares) is actually SSE, so the anova table is showing SSE. For null model the $\text{SSE} = 25.50$, the $\text{Df} = 7$. Recall for the Null model SSE is same as SST for the full model. So for the full model the $\text{SST} = 25.50$, the $\text{Df} = n - 1= 7$. Now the second row shows - SSE = 1.1153 - Df for SSE = 3 - SSR = 24.385 - Df for SSR = 4 With this it is possible to calculate other things asked in the question (do it!) Important is here $\text{SSR} = \text{SSE}_{R} - \text{SSE}$, because $\text{SSE}_R = \text{SST}$. ## Solution e. The $R^2$ in SLR is 0.653 and the $R^2$ in MLR is 0.956. So the $R^2$ is slightly higher in MLR, this is because we have added another variable in the model, so the $R^2$ will increase. The adjusted $R^2$ in MLR model is 0.898. ## Solution f. **answer of i..** Clearly in the MLR results, the `tv` and `newspaper` are significant both at 5\% level of significance. But magazines and leaflets are not significant, not even at 10\% level of significance. **answer of ii.** For overall significance testing we need to do the F-test. Again looking the summary of the regression output, it seems we can reject the joint hypothesis that all coefficients are $0$ can he rejected at 5% significance level. We can also see the value of the F-statistic and the p-value of this calculated F-statistics in the anova table (this is in answer d). **answer of iii.** In this case we have two restrictions, so we need to use restricted / unrestricted F-test. The null hypothesis is $$ H_0: \beta_3 = \beta_4 = 0 $$ The alternative is same as before. Again we need to use the anova table to do the F-test. First fit the model with the restrictions, this means we need to drop the `magazines` and `leaflets` from the model. ```{r} model_restrict &lt;- lm(revenue ~ tv + newspaper, data = Showtime) ``` Now compare this with the full model ```{r} anova( model_restrict, model_mlr) ``` Here $SSE_R - SSE = 0.94926 $ and $Df = 2$. So the F-statistic is $1.2766$ and the p-value is $0.312$. So we fail to reject the null hypothesis that $\beta_3 = \beta_4 = 0$ at 5% level of significance. So we can say that magazines and leaflets are not significant at 5% level of significance. Both coefficients are zero. So perhaps we can drop them. ## Solution g. This is the same prediction problem, but careful we need to divide the numbers by $1000$ because the model is in thousands of dollars. ```{r} test_data &lt;- data.frame(tv = 3500/1000, newspaper = 2300/1000, magazines = 1000/1000, leaflets = 500/1000) predict(model_mlr, newdata = test_data) ``` ## Solution h. The question is asking to provide a confidence interval for the mean of revenues when we have a value for the `tv` and `newspaper` advertisement. So we need to use the restricted model to predict (not the full model). ```{r} predict(model_restrict, newdata = data.frame(tv = 3500/1000, newspaper = 2300/1000), interval = "confidence") ``` ## solution i. Same question, but this time prediction interval ```{r} predict(model_restrict, newdata = data.frame(tv = 3500/1000, newspaper = 2300/1000), interval = "prediction") ``` ## solution j. Let's see the residual plot, here we need to plot the residuals against the fitted values. ```{r} plot(model_mlr$fitted.values, model_mlr$residuals) ``` # $\S.$ Problem 7 Soln This is a simulation exercise, simulation means the data world is in your control, you can generate the data. In this case we will generate a data with a given **Data Generating Process** (DGP). Here is the DGP $$ Y = 3 + 5X_1+ 2 X_2 + \epsilon $$ We call it DGP, because if we know the how $\epsilon$, $X_1$ and $X_2$ are generated, then we can generate the $Y$ variable. In this case we will generate the $X_1$, $X_2$ and $\epsilon$ from given distributions. Note that here $\beta_0 = 3$, $\beta_1 = 5$ and $\beta_2 = 2$. ```{r} # set the seed for reproducibility set.seed(1238818) # fix number of observations n &lt;- 50 # generate epsilon epsilon &lt;- rnorm(n, mean = 0, sd = sqrt(.25)) x2 &lt;- rnorm(n, mean = 0, sd = sqrt(.35)) x1 &lt;- runif(n, min = 0, max = 1) # generate y beta0 &lt;- 3 beta1 &lt;- 5 beta2 &lt;- 2 y &lt;- beta0 + beta1*x1 + beta2*x2 + epsilon # create a data frame sim_data &lt;- data.frame(sales = y, tv = x1, newspaper = x2) # view the data sim_data ``` Since we need to generate the data for $n = 50$, then $n = 1000$, then $n = 300$ and $n = 500$, rather than copying and pasting we will write a function first and generate the data just by changing the value of $n$ ```{r} # function to generate the data generate_data &lt;- function(n){ # generate epsilon epsilon &lt;- rnorm(n, mean = 0, sd = sqrt(.25)) x2 &lt;- rnorm(n, mean = 0, sd = sqrt(.35)) x1 &lt;- runif(n, min = 0, max = 1) # generate y y &lt;- 3 + 5*x1 + 2*x2 + epsilon # create a data frame sim_data &lt;- data.frame(y, x1, x2) # return the data return(sim_data) } ``` So every time we call this function it will return a data frame for us. ```{r} # generate the data for n = 50 # give the data a name data_50 &lt;- generate_data(50) # see the data data_50 ``` Let's do it for other values of $n$. ```{r} # generate the data for n = 1000 data_100 &lt;- generate_data(100) # generate the data for n = 300 data_300 &lt;- generate_data(300) # generate the data for n = 500 data_500 &lt;- generate_data(500) ``` Now finally we will fit the model for each of the data set and see the results. ```{r} # fit the model for n = 50 model_50 &lt;- lm(y ~ x1 + x2, data = data_50) summary(model_50) # fit the model for n = 1000 model_100 &lt;- lm(y ~ x1 + x2, data = data_100) summary(model_100) # fit the model for n = 300 model_300 &lt;- lm(y ~ x1 + x2, data = data_300) summary(model_300) # fit the model for n = 500 model_500 &lt;- lm(y ~ x1 + x2, data = data_500) summary(model_500) ``` For any fitted results we can also get the estimated coefficients by using the following command, ```{r} # get the estimated coefficients for n = 50 model_50$coefficients ``` Now we will generate the data for sample size $n = 50$, $1000$ times, fit the model and save $\hat{\beta}_1$ and plto the histogram. ```{r} # set the seed for reproducibility set.seed(1238818) # create an empty vector to save the results beta_1_hat_50 &lt;- c() # generate the data for n = 50, 1000 times for(i in 1:1000){ # generate the data data_50 &lt;- generate_data(50) # fit the model model_50 &lt;- lm(y ~ x1 + x2, data = data_50) # save the results beta_1_hat_50[i] &lt;- model_50$coefficients[2] } ``` Now we have all the estimated coefficients for $\hat{\beta}_1$ saved in the vector `beta_1_hat`, there will be $1000$ of them, since we did repeated sampling $1000$ times. Also note, sample size was 50 each times. Let's plot the histogram. ```{r} beta_1_hat_50 # there will be 1000 of the beta1hat from 1000 times repeated sampling ``` ```{r} # plot the histogram of 1000s beta1hats hist(beta_1_hat_50, breaks = 20) ``` This is the sampling distribution of $\hat{\beta}_1$ for $n = 50$. Notice it looks like Normal distribution. We can also calculate the standadrd error from this simulation, which is the standard deviation of the sampling distribution. ```{r} # calculate the standard error sd(beta_1_hat_50) ``` In this way you can also get the sampling distribution for $\hat{\beta}_0$ and $\hat{\beta}_2$. Note you need to use `model_50$coefficients[0]` command for $\hat{\beta}_0$ and `model_50$coefficients[3]` command for $\hat{\beta}_2$. Now will do the same for $n = 1000$ and compare the sampling distribution of $\hat{\beta}_1$ for $n = 50$ and $n = 1000$. ```{r} # set the seed for reproducibility set.seed(1238818) # create an empty vector to save the results beta_1_hat_1000 &lt;- c() # generate the data for n = 500, 1000 times for(i in 1:1000){ # generate the data data_1000 &lt;- generate_data(1000) # fit the model model_1000 &lt;- lm(y ~ x1 + x2, data = data_1000) # save the results beta_1_hat_1000[i] &lt;- model_1000$coefficients[2] } ``` let's calculate the standard error. ```{r} # calculate the standard error sd(beta_1_hat_1000) ``` Notice the standard error significantly reduced. This is the similar story like sample means, as we have more and more data, we have more accurate estimate. Now let's compare the two histograms ```{r} # plot the histogram for n = 50 hist(beta_1_hat_50, breaks = 10) # plot the histogram for n = 1000 hist(beta_1_hat_1000, breaks = 200) ``` Notice the histogram for $n = 1000$ is much more concentrated around the true value of $\beta_1 = 5$. # $\S.$ Problem 8 Soln We can include the interaction term by running following command, ```{r} # fit the model model_interaction &lt;- lm(revenue ~ tv*newspaper, data = Showtime) # view the results with stargazer package library(stargazer) stargazer(model_interaction, type = "text") ``` Again we can write the estimated model as, $$ \hat{revenue} = 86.531 - 1.089 \times tv - 0.667 \times newspaper + 0.724 \times (tv \times newspaper) $$ Now let's interpret the results. This is an interaction model, so we need to be careful. First let's slightly rewrite estimated model as $$ \hat{revenue} = 86.531 + (-1.089 + .724\; newspaper) \times tv - (0.667 \times newspaper) $$ This means if we increase the TV advertisement by 1 unit, the revenue is predicted to increase by $-1.089 + .724\; newspaper$ units. Notice this is a function of newspaper advertisement. So the effect of TV advertisement on revenue depends on the level of newspaper advertisement. # $\S.$ Problem 9 Soln This is for the auto data set that you already saw before. In this task we will do some non-linear regression, in particular polynomial regression with given degree. Interestingly we will see that the polynomial regression is actually a special case of multiple linear regression. ## Solution a. The first task is same, load the data. ```{r} # load the library, load the data library(readxl) Auto &lt;- read_excel("/home/tanvir/Documents/ownCloud/Git_Repos/EWU_repos/3_Fall_2023/eco_204/ewu-eco204.github.io/pdf_files/ps/04_mlr/Solutions/Auto.xlsx") ``` ```{r} summary(Auto) ``` surprisingly horsepower is showing some character, we need to fix it. ### Little bit of data cleaning (You can ignore this part) The summary stats show there is a `char` which means character or text in horepower. This is a problem. let's see the data for horsepower, ```{r} sort(Auto$horsepower) ``` there is a actually a question mark, this is actually missing data. We need to fix it. Following command will remove the question marks, ```{r} # there are many ways also with a package dplyr, but here is a simple way to do this Auto &lt;- Auto[Auto$horsepower != "?", ] ``` However still it's character, we need to convert it to numeric, the function in R is `as.numeric()`, for example ```{r} "134" as.numeric("134") ``` Let's now do it for horsepower ```{r} Auto$horsepower &lt;- as.numeric(Auto$horsepower) ``` Now let's see the summary statistics again ```{r} summary(Auto) ``` Looks good Let's plot the data, ```{r} plot(Auto$horsepower, Auto$mpg) ``` So the pattern shows some non-linearities ## Running Polynomial Regression Now let's perfoem polynomial regression. We will use the `I()` to incorporate polynomials in `lm()` ```{r} quad_fit &lt;- lm(mpg ~ horsepower + I(horsepower^2), data = Auto) library(stargazer) stargazer(quad_fit, type = "text") ``` ## Plot the fitted line ```{r} plot(Auto$horsepower, Auto$mpg) lines(sort(Auto$horsepower), fitted(quad_fit)[order(Auto$horsepower)], col = "red", lwd = 2) ``` What if we want to perform cubic regression? We can do that too, ```{r} cubic_fit &lt;- lm(mpg ~ horsepower + I(horsepower^2) + I(horsepower^3), data = Auto) stargazer(cubic_fit, type = "text") ``` Plot all the lines ```{r} plot(Auto$horsepower, Auto$mpg) lines(sort(Auto$horsepower), fitted(quad_fit)[order(Auto$horsepower)], col = "red", lwd = 2) lines(sort(Auto$horsepower), fitted(cubic_fit)[order(Auto$horsepower)], col = "blue", lwd = 2) ``` The improvement is not so much, we can also do it for higher order polynomia, in this way with `I()` in `lm()`. But there is another function in R called `poly()` that can do it for us. ## Polynomial Regression with `poly()` ```{r} poly_fit &lt;- lm(mpg ~ poly(horsepower, 3, raw = TRUE ), data = Auto) stargazer(poly_fit, type = "text") ``` # $\S$. Problem 11 Soln ## Load the data The first task is same, load the data. ```{r} Auto &lt;- read_excel("/home/tanvir/Documents/ownCloud/Git_Repos/EWU_repos/3_Fall_2023/eco_204/ewu-eco204.github.io/pdf_files/ps/04_mlr/Solutions/Auto_clean.xlsx") ``` ```{r} summary(Auto) ``` Origin variable should be a factor, let's fix it ```{r} Auto$origin &lt;- factor(Auto$origin, labels = c("USA", "Europe", "Japan")) summary(Auto) ``` ```{r} # without interaction model1 &lt;- lm(mpg ~ horsepower + origin, data = Auto) summary(model1) # with interaction model2 &lt;- lm(mpg ~ horsepower + origin + origin*horsepower, data = Auto) summary(model2) ``` ```{r, include = FALSE} # You can ignore everything from here.... knitr::purl("PS-4_Soln.Rmd") ``` </p></body></html>