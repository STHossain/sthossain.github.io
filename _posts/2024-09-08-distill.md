---
layout: distill
title: Subgaussian Random Variables
description: Some details on Subgaussian
tags: subgaussain_random_variables 
categories: concentration_bounds
giscus_comments: true
date: 2024-09-08
featured: false

# authors:
#   - name: Albert Einstein
#     url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#     affiliations:
#       name: IAS, Princeton
#   - name: Boris Podolsky
#     url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#     affiliations:
#       name: IAS, Princeton
#   - name: Nathan Rosen
#     url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#     affiliations:
#       name: IAS, Princeton

bibliography: 2024-09-08-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).


# toc:
#   - name: Equations
#     # if a section has subsections, you can add them as follows:
#     # subsections:
#     #   - name: Example Child Subsection 1
#     #   - name: Example Child Subsection 2
#   - name: Citations
#   - name: Footnotes
#   - name: Code Blocks
#   - name: Interactive Plots
#   - name: Layouts
#   - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }



---

When I started to read multi armed bandit (a.k.a. sequential treatment allocation) literature, had to deal with concentration bounds (somehow I love and hate this topic). The key idea is essentially bounding the tail probabilities of the random variables. Since I myself sometimes forget many details, I decided to write ~~everyday~~, time to time (even if it's very little). Hopefully by this I will learn many things and this helps people who has struggled like me. 

This is the first post of the series of posts on concentration bounds that I will try to write. There are different but equivalent ways to define a subgaussain random variable, I will follow following definitions, 

## Basic Definitions and Properties

========================================================================

**Definition 1 [Subgaussain random variable]:** 

We say a random variable is a subgaussain random variable with subgausian parameter $\sigma$, we write $X \sim s\mathcal{G}(\sigma)$, if following holds

$$
\mathbb{E}(e^{\lambda X}) \leq e^{\frac{1}{2} \lambda^2 \sigma^2} 
$$

========================================================================



Now I will quickly state some important properties of subgaussian random variables, that is possible to derive using this definition,



**Proprties of Subgaussian Random Variables**

<ol>

<li> Using Markov's inequality, we can show that for any $\epsilon > 0$,

  $$
   \mathbb{P}(X \geq \epsilon) \leq e^{-\frac{\epsilon^2}{2 \sigma^2}} \quad \text{ and } \quad  \mathbb{P}(|X| \geq \epsilon) \leq 2 e^{-\frac{\epsilon^2}{2 \sigma^2}}
  $$
  
this is often called **Chernoff bound**, and is very useful in many places. The important thing to note where subgaussian paramter $\sigma^2$ is in the bound. 

</li>


<li>

If $X_i \sim s\mathcal{G}(\sigma_i)$ and all of the random variables are independent, then 
  
  $$ 
    \sum_{i = 1}^{n} X_i \sim s\mathcal{G}\left(\sqrt{\sum_{i = 1}^{n}\sigma_i^2}\right) 
  $$

A special case of this is when all $\sigma_i$ are same, i.e., $\sigma_i = \sigma$ then we have a iid setup, or we have $X_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma)$, and we have  $\sum_{i = 1}^{n} X_i \sim s\mathcal{G}(\sqrt{n} \sigma)$

</li>


<li>

If $X \sim s \mathcal{G}(\sigma)$, then 
  
  $$ 
    a X \sim s \mathcal{G}(|a|\sigma)
  $$

</li>



<li>

Many random variables are subgaussian, for example, 

  <ul>
    
  <li>
  
  If $X \sim \mathcal{N}(\mu, \sigma^2)$, then 
  
  $$ 
      X - \mu \sim s\mathcal{G}(\sigma)
  $$ 
  
  so in this case the $\sigma$ is the standard deviation of the normal random variable. 
  
  </li>
  
  <li>
  
  Bounded Random Variable $X$, for example $X \in [a, b]$ then  
  
  $$ 
  X - \mu \sim s\mathcal{G}\left(\cfrac{b-a}{2}\right)
  $$ 
  </li>

  </ul>

</li>


</ol>

## Hoeffding's Bound

The idea of the Hoeffding's Bound is to get a tail bound for the average / sum of the subgaussian random variables, i.e., we are looking for 

$$
\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \text{?}
$$

or more simply,

$$
\mathbb{P}\left[ \overline{X}_{n} - \mu \geq \epsilon \right] \leq \text{?}
$$


In this case the key step is to first derive the subgaussian parameter of $\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right)$ or $\overline{X}_n - \mu$. This might vary depending on the setup. But if we know this, for example, if we know

$$
\overline{X}_n - \mu \sim s\mathcal{G}\left(\tilde{\sigma}_{S}\right)
$$

then applying Chernoff bound, we get

$$
\mathbb{P}\left[\overline{X}_n - \mu \geq \epsilon \right] \leq \exp{\left(-\frac{1}{2} \cdot \frac{\epsilon^2}{ \tilde{\sigma}_S^2}\right)}
$$

We will see some specific examples first and then outline the porposition from the book by Wainwright <d-cite key="wainwright2019high"></d-cite>. 

---
**[Example 1]**

If we have $X_i - \mu_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma_i)$, then we have

$$ 
     \frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \sim s\mathcal{G}\left(\frac{1}{n}\sqrt{\sum_{i = 1}^{n}\sigma_i^2}\right)
$$ 
  
and applying Chernoff bound, we get for all $\epsilon \geq 0$,

$$
\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \exp{\left(- \frac{1}{2}\cdot \frac{n^2 \epsilon^2}{\sum_{i=1}^n \sigma_i^2}\right)}
$$

---

**[Example 2]**

if we have $X_i - \mu \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma)$, then we have

$$
\overline{X}_n - \mu \sim s\mathcal{G}\left(\frac{\sigma}{\sqrt{n}}\right) $$

And applying Chernoff bound, we get for all $\epsilon \geq 0$,
$$
\mathbb{P}(\overline{X}_n - \mu \geq \epsilon) \leq \exp{\left(-\frac{1}{2}\frac{n \epsilon^2}{\sigma^2}\right)} 
$$

---

**[Example 3]**

if all $X_i$ are bounded random variables where $X_i \in [a_i, b_i]$ , then we have

$$
   \frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \sim s\mathcal{G}\left(\frac{1}{n}\sqrt{ \sum_{i =1}^{n} \frac{(b_i - a_i)^2}{4} }\right)  
$$

And applying Chernoff bound, we get for all $\epsilon \geq 0$,

$$
\mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \exp{\left(- \frac{1}{2}\cdot \frac{n^2 \epsilon^2}{\sum_{i=1}^n \frac{(b_i - a_i)^2}{4}}\right)} = \exp{\left(- \frac{2 n^2 \epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)}  
$$
---

**[Example 4]**

Again another special case when $X_i \in [a, b]$, then we have

$$
\overline{X}_n - \mu \sim s\mathcal{G}\left(\frac{1}{\sqrt{n}}\frac{(b-a)}{2}\right) 
$$
 
And applying Chernoff bound, we get for all $\epsilon \geq 0$,

$$
\mathbb{P}(\overline{X}_n - \mu \geq \epsilon) \leq \exp{\left(  -\frac{2 n \epsilon^2}{ (b-a)^2 }  \right)} 

$$
---


These are all examples of Hoeffding's bound. Following is the general form of the Hoeffding's bound, proposition is taken from the book of Wainwright <d-cite key="wainwright2019high"></d-cite>



> **Proposition [Hoeffding bound**]: Suppose we have a sequence $X_i - \mu_i \overset{i.i.d.}{\sim} s\mathcal{G}(\sigma_i)$, then for all $\epsilon \geq 0$, we have 
>
>$$    
    \mathbb{P}\left[\frac{1}{n}\sum_{i=1}^n\left(X_i-\mu_i\right) \geq \epsilon \right] \leq \exp{\left(- \frac{1}{2}\cdot \frac{n^2 \epsilon^2}{\sum_{i=1}^n \sigma_i^2}\right)}
$$

Finally if $X_i \in [0, 1]$, for all $i$ and they are independent, we have a very simple bound,

$$
\mathbb{P}(\overline{X}_n - \mu \geq \epsilon) \leq \exp{\left(  - 2 n \epsilon^2  \right)}
$$
